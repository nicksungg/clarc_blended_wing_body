{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nicksung/Desktop/nicksung/bwb/data_v4\n"
     ]
    }
   ],
   "source": [
    "cd \"/home/nicksung/Desktop/nicksung/bwb/data_v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "#############################\n",
    "# 1) Dataset & Data Loading #\n",
    "#############################\n",
    "\n",
    "class DesignDataset(Dataset):\n",
    "    \"\"\"\n",
    "    (Identical to what you used in training)\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, hdf5_path, num_designs=99):\n",
    "        super().__init__()\n",
    "        self.num_designs = num_designs\n",
    "\n",
    "        # 1a) Read CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.rename(columns={\"CASE NO\": \"case_index\"}, inplace=True)\n",
    "        df = df[~df[\"case_index\"].between(390, 399)]  # remove bad cases\n",
    "\n",
    "        # 1b) Open HDF5\n",
    "        h5f = h5py.File(hdf5_path, \"r\")\n",
    "        points_grp = h5f[\"points\"]\n",
    "        cp_grp     = h5f[\"cp\"]\n",
    "        cfx_grp    = h5f[\"cf_x\"]\n",
    "        cfy_grp    = h5f[\"cf_y\"]\n",
    "        cfz_grp    = h5f[\"cf_z\"]\n",
    "\n",
    "        shape_cols  = ['B1', 'B2', 'B3', 'C1', 'C2', 'C3', 'C4', 'S1', 'S2', 'S3']\n",
    "        flight_cols = ['alt_kft', 'Re_L', 'M_inf', 'alpha_deg', 'beta_deg']\n",
    "\n",
    "        # Compute normalization for shape + flight\n",
    "        shape_mean, shape_std = df[shape_cols].mean().values, df[shape_cols].std().values\n",
    "        flight_mean, flight_std = df[flight_cols].mean().values, df[flight_cols].std().values\n",
    "        shape_std[shape_std == 0] = 1\n",
    "        flight_std[flight_std == 0] = 1\n",
    "\n",
    "        # Find min/max coords and gather all outputs for normalization\n",
    "        coord_min = np.array([ float(\"inf\"),  float(\"inf\"),  float(\"inf\")])\n",
    "        coord_max = np.array([-float(\"inf\"), -float(\"inf\"), -float(\"inf\")])\n",
    "        all_outputs = []\n",
    "        for case_id in df[\"case_index\"]:\n",
    "            ds_key = f\"case_{case_id:03d}\"\n",
    "            if ds_key in points_grp:\n",
    "                coords = points_grp[ds_key][()]\n",
    "                coord_min = np.minimum(coord_min, coords.min(axis=0))\n",
    "                coord_max = np.maximum(coord_max, coords.max(axis=0))\n",
    "\n",
    "                cp_data  = cp_grp[ds_key][()]\n",
    "                cfx_data = cfx_grp[ds_key][()]\n",
    "                cfy_data = cfy_grp[ds_key][()]\n",
    "                cfz_data = cfz_grp[ds_key][()]\n",
    "                all_outputs.append(np.stack([cp_data, cfx_data, cfy_data, cfz_data], axis=-1))\n",
    "\n",
    "        all_outputs = np.concatenate(all_outputs, axis=0)  # (TotalPoints, 4)\n",
    "        output_mean = all_outputs.mean(axis=0)\n",
    "        output_std  = all_outputs.std(axis=0)\n",
    "        output_std[output_std == 0] = 1\n",
    "\n",
    "        # Store for use in unnormalization\n",
    "        self.shape_mean, self.shape_std = shape_mean, shape_std\n",
    "        self.flight_mean, self.flight_std = flight_mean, flight_std\n",
    "        self.coord_min, self.coord_max = coord_min, coord_max\n",
    "        self.output_mean, self.output_std = output_mean, output_std\n",
    "\n",
    "        # Build the final dataset structure\n",
    "        tmp_designs = {i: [] for i in range(self.num_designs)}\n",
    "\n",
    "        for row in df.itertuples(index=False):\n",
    "            cidx = row.case_index\n",
    "            design_idx = cidx // 10\n",
    "            if design_idx >= self.num_designs:\n",
    "                continue\n",
    "\n",
    "            ds_key = f\"case_{cidx:03d}\"\n",
    "            if ds_key not in points_grp:\n",
    "                continue\n",
    "\n",
    "            # flight+shape => normalized\n",
    "            flight_arr = np.array([row.alt_kft, row.Re_L, row.M_inf, row.alpha_deg, row.beta_deg], dtype=np.float32)\n",
    "            shape_arr  = np.array([row.B1, row.B2, row.B3, row.C1, row.C2, row.C3, row.C4, row.S1, row.S2, row.S3], dtype=np.float32)\n",
    "            flight_arr = (flight_arr - flight_mean) / flight_std\n",
    "            shape_arr  = (shape_arr  - shape_mean)  / shape_std\n",
    "            full_cond  = np.concatenate([flight_arr, shape_arr]).astype(np.float32)\n",
    "\n",
    "            # coords => normalized\n",
    "            pts_coords = points_grp[ds_key][()]\n",
    "            pts_coords = 2.0 * (pts_coords - coord_min) / (coord_max - coord_min) - 1.0\n",
    "\n",
    "            # coeffs => normalized\n",
    "            cp_vals  = (cp_grp[ds_key][()]  - output_mean[0]) / output_std[0]\n",
    "            cfx_vals = (cfx_grp[ds_key][()] - output_mean[1]) / output_std[1]\n",
    "            cfy_vals = (cfy_grp[ds_key][()] - output_mean[2]) / output_std[2]\n",
    "            cfz_vals = (cfz_grp[ds_key][()] - output_mean[3]) / output_std[3]\n",
    "            coeffs_4 = np.stack([cp_vals, cfx_vals, cfy_vals, cfz_vals], axis=-1)\n",
    "\n",
    "            tmp_designs[design_idx].append({\n",
    "                'flight_cond': full_cond,\n",
    "                'points': pts_coords.astype(np.float32),\n",
    "                'coeffs': coeffs_4.astype(np.float32)\n",
    "            })\n",
    "\n",
    "        # Keep only complete designs (10 flight conditions)\n",
    "        self.design_info = [tmp_designs[i] for i in range(self.num_designs) if len(tmp_designs[i]) == 10]\n",
    "        self.num_designs = len(self.design_info)\n",
    "\n",
    "        h5f.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_designs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.design_info[idx]\n",
    "\n",
    "\n",
    "################################################\n",
    "# 2) Train/Val/Test Splitting (Design-level)\n",
    "################################################\n",
    "def split_designs(full_dataset, train_ratio=0.8, val_ratio=0.1):\n",
    "    n_total = len(full_dataset)\n",
    "    n_train = int(train_ratio * n_total)\n",
    "    n_val   = int(val_ratio   * n_total)\n",
    "    n_test  = n_total - n_train - n_val\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        full_dataset, [n_train, n_val, n_test],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "######################################\n",
    "# 3) Define the FiLM-based MLP Model #\n",
    "######################################\n",
    "class FiLMModulation(nn.Module):\n",
    "    def __init__(self, cond_dim, hidden_dim=256, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        # scale+shift => 2 * hidden_dim * (num_layers - 1)\n",
    "        self.num_mod_params = 2 * hidden_dim * (num_layers - 1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(cond_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.num_mod_params)\n",
    "        )\n",
    "\n",
    "    def forward(self, cond):\n",
    "        out = self.fc(cond)\n",
    "        chunk_size = self.hidden_dim*(self.num_layers-1)\n",
    "        gamma = out[:, :chunk_size]\n",
    "        beta  = out[:,  chunk_size:]\n",
    "        return gamma, beta\n",
    "\n",
    "\n",
    "class ModulatedMLP(nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=4, hidden_dim=256, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, coords, gamma, beta):\n",
    "        chunk_size = self.hidden_dim\n",
    "        h = coords\n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.layers[i](h)\n",
    "            h = torch.relu(h)\n",
    "            g_i = gamma[:, i*chunk_size:(i+1)*chunk_size]\n",
    "            b_i = beta[:,  i*chunk_size:(i+1)*chunk_size]\n",
    "            h   = g_i * h + b_i\n",
    "        out = self.layers[-1](h)  # final\n",
    "        return out\n",
    "\n",
    "class AeroFiLMNet(nn.Module):\n",
    "    def __init__(self, cond_dim=15, coord_dim=3, output_dim=4, hidden_dim=256, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.modulation_net = FiLMModulation(cond_dim, hidden_dim, num_layers)\n",
    "        self.mlp = ModulatedMLP(coord_dim, output_dim, hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, coords, cond):\n",
    "        gamma, beta = self.modulation_net(cond)\n",
    "        out = self.mlp(coords, gamma, beta)\n",
    "        return out\n",
    "\n",
    "\n",
    "############################################################\n",
    "# 4) Utility: load a trained model checkpoint\n",
    "############################################################\n",
    "def load_trained_model(model_path, device='cuda'):\n",
    "    cond_dim   = 15\n",
    "    coord_dim  = 3\n",
    "    output_dim = 4\n",
    "    hidden_dim = 256\n",
    "    num_layers = 4\n",
    "    model = AeroFiLMNet(cond_dim, coord_dim, output_dim, hidden_dim, num_layers)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# 5) Utility: get all points for a given design+case\n",
    "###########################################################\n",
    "def get_all_points_for_design_case(dataset, design_idx, case_idx):\n",
    "    \"\"\"\n",
    "    Returns (coords_norm, coeffs_norm, cond_norm),\n",
    "      coords_norm: (N, 3)\n",
    "      coeffs_norm: (N, 4)\n",
    "      cond_norm   : (15,)\n",
    "    \"\"\"\n",
    "    data_dict = dataset.design_info[design_idx][case_idx]\n",
    "    coords = data_dict['points']      # shape (N,3) normalized\n",
    "    coeffs = data_dict['coeffs']      # shape (N,4) normalized\n",
    "    cond   = data_dict['flight_cond'] # shape (15,) normalized\n",
    "    return coords, coeffs, cond\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# 6) Inverse normalization for the aerodynamic coefficients\n",
    "#############################################################\n",
    "def unnormalize_coeffs(coeffs_norm, dataset):\n",
    "    \"\"\"\n",
    "    coeffs_norm: shape (N,4), each row = [cp, cfx, cfy, cfz] (normalized)\n",
    "    returns unnormalized (N,4).\n",
    "    \"\"\"\n",
    "    cp  = coeffs_norm[:, 0] * dataset.output_std[0] + dataset.output_mean[0]\n",
    "    cfx = coeffs_norm[:, 1] * dataset.output_std[1] + dataset.output_mean[1]\n",
    "    cfy = coeffs_norm[:, 2] * dataset.output_std[2] + dataset.output_mean[2]\n",
    "    cfz = coeffs_norm[:, 3] * dataset.output_std[3] + dataset.output_mean[3]\n",
    "    return np.column_stack([cp, cfx, cfy, cfz])\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# 7) Compute the unnormalized MSE across the entire test set\n",
    "#########################################################\n",
    "def compute_unnormalized_mse(model, dataset, test_design_indices, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loops over the test design indices & each design's 10 flight conditions,\n",
    "    gathers all points, predicts & unnormalizes them, compares to GT (also unnormalized),\n",
    "    and accumulates total squared error. Returns final MSE for each dimension (cp, cf_x, cf_y, cf_z).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mse_accum = np.zeros(4, dtype=np.float64)  # sum of squared errors for cp, cfx, cfy, cfz\n",
    "    count     = 0                              # total number of points\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d_idx in test_design_indices:\n",
    "            # Each design has 10 flight conditions\n",
    "            for c_idx in range(10):\n",
    "                coords_norm, gt_norm, cond_norm = get_all_points_for_design_case(dataset, d_idx, c_idx)\n",
    "\n",
    "                # Convert to torch\n",
    "                coords_torch = torch.from_numpy(coords_norm).float().to(device)\n",
    "                cond_torch   = torch.from_numpy(cond_norm).float().unsqueeze(0).to(device)\n",
    "                cond_torch   = cond_torch.expand(coords_torch.shape[0], -1)\n",
    "\n",
    "                preds_norm = model(coords_torch, cond_torch)  # (N,4)\n",
    "                preds_norm = preds_norm.cpu().numpy()\n",
    "\n",
    "                # Unnormalize\n",
    "                preds_unnorm = unnormalize_coeffs(preds_norm, dataset)  # (N,4)\n",
    "                gt_unnorm    = unnormalize_coeffs(gt_norm, dataset)      # (N,4)\n",
    "\n",
    "                # Accumulate SSE\n",
    "                diff = preds_unnorm - gt_unnorm  # (N,4)\n",
    "                squared_diff = diff**2           # (N,4)\n",
    "                mse_accum += np.sum(squared_diff, axis=0)  # sum over points\n",
    "                count += coords_norm.shape[0]\n",
    "\n",
    "    # Now compute MSE per dimension\n",
    "    mse_per_dim = mse_accum / count  # (4,) => [MSE_cp, MSE_cfx, MSE_cfy, MSE_cfz]\n",
    "    return mse_per_dim\n",
    "\n",
    "\n",
    "################################################\n",
    "# 8) Main\n",
    "################################################\n",
    "def main():\n",
    "    # File paths\n",
    "    csv_file   = \"/home/nicksung/Desktop/nicksung/bwb/data_v4/parametric/modified_merged_params.csv\"\n",
    "    hdf5_file  = \"/home/nicksung/Desktop/nicksung/bwb/data_v4/surface_point/surface_data.hdf5\"\n",
    "    model_file = \"/home/nicksung/Desktop/nicksung/bwb/film_model_saved_weights/film_model.pth\"   # <-- your saved checkpoint\n",
    "    device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Build the full dataset\n",
    "    full_dataset = DesignDataset(csv_file, hdf5_file, num_designs=99)\n",
    "\n",
    "    # 2) Split into train/val/test\n",
    "    train_ds, val_ds, test_ds = split_designs(full_dataset, train_ratio=0.8, val_ratio=0.1)\n",
    "    print(\"Train designs:\", len(train_ds), \"Val designs:\", len(val_ds), \"Test designs:\", len(test_ds))\n",
    "\n",
    "    # 3) We only need the design indices of test_ds\n",
    "    #    random_split() returns Subset objects that store indices in .indices\n",
    "    test_indices = test_ds.indices  # This is a list of design indices in the full dataset\n",
    "\n",
    "    # 4) Load the trained model\n",
    "    model = load_trained_model(model_file, device=device)\n",
    "\n",
    "    # 5) Compute unnormalized MSE on all test designs\n",
    "    mse_cp, mse_cfx, mse_cfy, mse_cfz = compute_unnormalized_mse(model, full_dataset, test_indices, device=device)\n",
    "\n",
    "    print(\"Unnormalized MSE for test set:\")\n",
    "    print(f\"  cp  : {mse_cp:.6e}\")\n",
    "    print(f\"  cf_x: {mse_cfx:.6e}\")\n",
    "    print(f\"  cf_y: {mse_cfy:.6e}\")\n",
    "    print(f\"  cf_z: {mse_cfz:.6e}\")\n",
    "\n",
    "    # If desired, you can also compute an overall average MSE across the 4 dims:\n",
    "    avg_mse = (mse_cp + mse_cfx + mse_cfy + mse_cfz) / 4.0\n",
    "    print(f\"Average unnormalized MSE across (cp, cf_x, cf_y, cf_z): {avg_mse:.6e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train designs: 78 Val designs: 9 Test designs: 11\n",
      "Unnormalized MSE for test set:\n",
      "  cp  : 2.938683e-02\n",
      "  cf_x: 1.023803e-06\n",
      "  cf_y: 5.246763e-07\n",
      "  cf_z: 9.456416e-07\n",
      "Average unnormalized MSE across (cp, cf_x, cf_y, cf_z): 7.347330e-03\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_unnormalized_stats(dataset):\n",
    "    \"\"\"\n",
    "    Loops over all designs & flight conditions in `dataset.design_info`.\n",
    "    For each (design, case), retrieves the normalized coeffs -> unnormalizes\n",
    "    -> stores them in big arrays.\n",
    "\n",
    "    Then prints out mean, std, min, max for each of (cp, cf_x, cf_y, cf_z).\n",
    "    \"\"\"\n",
    "    all_cp   = []\n",
    "    all_cfx  = []\n",
    "    all_cfy  = []\n",
    "    all_cfz  = []\n",
    "\n",
    "    for d_idx in range(dataset.num_designs):\n",
    "        # Each design_info[d_idx] is a list of 10 flight conditions\n",
    "        design_cases = dataset.design_info[d_idx]\n",
    "        for case_data in design_cases:\n",
    "            # case_data is a dict with 'points', 'coeffs' (normalized), 'flight_cond'\n",
    "            coeffs_norm = case_data['coeffs']  # shape (N,4)\n",
    "\n",
    "            # Unnormalize them:\n",
    "            #   cp_unnorm = cp_norm * std_cp + mean_cp, etc.\n",
    "            cp_unnorm  = coeffs_norm[:, 0] * dataset.output_std[0] + dataset.output_mean[0]\n",
    "            cfx_unnorm = coeffs_norm[:, 1] * dataset.output_std[1] + dataset.output_mean[1]\n",
    "            cfy_unnorm = coeffs_norm[:, 2] * dataset.output_std[2] + dataset.output_mean[2]\n",
    "            cfz_unnorm = coeffs_norm[:, 3] * dataset.output_std[3] + dataset.output_mean[3]\n",
    "\n",
    "            all_cp.append(cp_unnorm)\n",
    "            all_cfx.append(cfx_unnorm)\n",
    "            all_cfy.append(cfy_unnorm)\n",
    "            all_cfz.append(cfz_unnorm)\n",
    "\n",
    "    # Concatenate all points from all designs/cases\n",
    "    all_cp   = np.concatenate(all_cp)\n",
    "    all_cfx  = np.concatenate(all_cfx)\n",
    "    all_cfy  = np.concatenate(all_cfy)\n",
    "    all_cfz  = np.concatenate(all_cfz)\n",
    "\n",
    "    # Compute basic stats\n",
    "    print(\"==== Unnormalized Stats for Entire Dataset ====\")\n",
    "    print(f\"CP   : mean={all_cp.mean():.5f},  std={all_cp.std():.5f},  min={all_cp.min():.5f},  max={all_cp.max():.5f}\")\n",
    "    print(f\"CF_x : mean={all_cfx.mean():.5f}, std={all_cfx.std():.5f}, min={all_cfx.min():.5f}, max={all_cfx.max():.5f}\")\n",
    "    print(f\"CF_y : mean={all_cfy.mean():.5f}, std={all_cfy.std():.5f}, min={all_cfy.min():.5f}, max={all_cfy.max():.5f}\")\n",
    "    print(f\"CF_z : mean={all_cfz.mean():.5f}, std={all_cfz.std():.5f}, min={all_cfz.min():.5f}, max={all_cfz.max():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw means from the dataset: [-1.14445232e-01  3.33085008e-03 -1.96375195e-06  5.94268788e-04]\n",
      "Raw stds from the dataset : [0.45769806 0.00254975 0.00147762 0.00324235]\n",
      "==== Unnormalized Stats for Entire Dataset ====\n",
      "CP   : mean=-0.11456,  std=0.45777,  min=-7.95721,  max=1.68977\n",
      "CF_x : mean=0.00333, std=0.00255, min=-0.01830, max=0.03099\n",
      "CF_y : mean=-0.00000, std=0.00148, min=-0.03001, max=0.03182\n",
      "CF_z : mean=0.00059, std=0.00324, min=-0.01891, max=0.09298\n"
     ]
    }
   ],
   "source": [
    "csv_file   = \"/home/nicksung/Desktop/nicksung/bwb/data_v4/parametric/modified_merged_params.csv\"\n",
    "hdf5_file  = \"/home/nicksung/Desktop/nicksung/bwb/data_v4/surface_point/surface_data.hdf5\"\n",
    "\n",
    "# 1) Build the dataset (this computes all your normalization info)\n",
    "full_dataset = DesignDataset(csv_file, hdf5_file, num_designs=99)\n",
    "\n",
    "# 2) Print the stored means & stds directly:\n",
    "print(\"Raw means from the dataset:\", full_dataset.output_mean)  # shape (4,)\n",
    "print(\"Raw stds from the dataset :\", full_dataset.output_std)   # shape (4,)\n",
    "\n",
    "# 3) Use our helper function to see the distribution (min, max, etc.)\n",
    "compute_unnormalized_stats(full_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
