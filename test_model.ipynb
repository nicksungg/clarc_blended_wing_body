{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | CHUNK_SIZE=200000\n",
      "[dataset_v4] designs kept: 2498, skipped rows: 0\n",
      "Test designs (meshes): 2498\n",
      "\n",
      "Evaluated 116,130,401 points across all cases in 0.2 min\n",
      "=== TRUE shape metrics (test, ALL points) ===\n",
      "cp   | MSE=4.176333e-03  MAE=2.146596e-02  RelL1=7.111372e-02  RelL2=1.419427e-02\n",
      "cf_x | MSE=6.501164e-06  MAE=6.824867e-04  RelL1=1.464331e-01  RelL2=9.153393e-02\n",
      "cf_z | MSE=1.654494e-06  MAE=3.807442e-04  RelL1=1.937697e-01  RelL2=4.535215e-02\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Evaluate FiLMNet on TEST split using *all points of all cases* (TRUE shape only).\n",
    "\n",
    "- Loads train-time norm_stats.json if available (reproducible normalization).\n",
    "- Otherwise, computes stats from TRAIN CSV+HDF5 as a fallback.\n",
    "- Iterates every design -> every case -> all points (chunked) with NO subsampling.\n",
    "- Reports MSE, MAE, RelL1, RelL2 for cp, cf_x, cf_z.\n",
    "\n",
    "Requires:\n",
    "  - dataset.py providing UnifiedDesignDataset\n",
    "  - film_model_v1.py providing FiLMNet\n",
    "\"\"\"\n",
    "\n",
    "import os, json, time, numpy as np, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Your modules ---\n",
    "from dataset import UnifiedDesignDataset\n",
    "from film_model_v1 import FiLMNet\n",
    "\n",
    "# ========================== PATHS ==========================\n",
    "TRAIN_CSV = \"/home/nicksung/Desktop/nicksung/bwb_pp/data/case_with_geom_params.csv\"\n",
    "TRAIN_H5  = \"/home/nicksung/Desktop/nicksung/bwb_pp/data/surface_data.hdf5\"\n",
    "\n",
    "TEST_CSV  = \"/home/nicksung/Desktop/nicksung/bwb_pp/test_data/case_with_geom_params.csv\"\n",
    "TEST_H5   = \"/home/nicksung/Desktop/nicksung/bwb_pp/test_data/surface_data_test.hdf5\"\n",
    "\n",
    "NORM_JSON   = \"norm_stats.json\"  # same file used during training\n",
    "MODEL_PATH  = \"/home/nicksung/Desktop/nicksung/bwb_pp/checkpoints/film_best.pth\"\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHUNK_SIZE  = int(os.getenv(\"EVAL_CHUNK\", \"200000\"))  # points per forward pass\n",
    "\n",
    "# ===================== HELPERS ============================\n",
    "def load_norm_stats_or_compute(train_csv, train_h5, norm_json_path):\n",
    "    \"\"\"\n",
    "    Prefer loading train-time stats from JSON for exact reproducibility.\n",
    "    If not found, compute from TRAIN data via dataset logic and save them.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(norm_json_path):\n",
    "        raw = json.load(open(norm_json_path, \"r\"))\n",
    "        return {k: np.array(v, dtype=np.float32) for k, v in raw.items()}\n",
    "\n",
    "    tmp_train = UnifiedDesignDataset(csv_path=train_csv, hdf5_path=train_h5, norm_stats=None, mode=\"train\")\n",
    "    ns = tmp_train.norm_stats\n",
    "    norm_stats = {k: np.array(v, dtype=np.float32) for k, v in ns.items()}\n",
    "    try:\n",
    "        with open(norm_json_path, \"w\") as f:\n",
    "            json.dump({k: v.tolist() for k, v in norm_stats.items()}, f, indent=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return norm_stats\n",
    "\n",
    "def load_model(model_path, device=DEVICE):\n",
    "    # Match your training config exactly\n",
    "    model = FiLMNet(cond_dim=13, coord_dim=6, output_dim=3,\n",
    "                    hidden_dim=256, num_layers=4, extra_layers=3)\n",
    "    sd = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(sd)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_test_metrics_full(model, test_ds, device=DEVICE, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"\n",
    "    Iterates every design -> every case -> all points (chunked), no subsampling.\n",
    "    Returns dicts for MSE, MAE, RelL1, RelL2 keyed by ['cp', 'cf_x', 'cf_z'].\n",
    "    \"\"\"\n",
    "    # Accumulators (per-channel: cp, cf_x, cf_z)\n",
    "    sse = np.zeros(3, dtype=np.float64)      # sum of squared errors\n",
    "    sae = np.zeros(3, dtype=np.float64)      # sum of absolute errors\n",
    "    sum_gt_abs = np.zeros(3, dtype=np.float64)\n",
    "    sum_gt_sq  = np.zeros(3, dtype=np.float64)\n",
    "    total_pts  = 0\n",
    "\n",
    "    # Convenience for un-normalization using the same stats the dataset exposes\n",
    "    out_mu = torch.tensor(test_ds.output_mean, device=device).view(1, 3)\n",
    "    out_sd = torch.tensor(test_ds.output_std,  device=device).view(1, 3)\n",
    "\n",
    "    t_start = time.time()\n",
    "    n_designs = len(test_ds)\n",
    "    for d_idx in range(n_designs):\n",
    "        design = test_ds[d_idx]  # list of case dicts for this mesh/design\n",
    "        for sample in design:\n",
    "            coords  = torch.from_numpy(sample[\"points\"]).to(device=device, dtype=torch.float32)   # [N,6]\n",
    "            targets = torch.from_numpy(sample[\"coeffs\"]).to(device=device, dtype=torch.float32)   # [N,3] (normalized)\n",
    "            cond    = torch.from_numpy(sample[\"flight_cond\"]).to(device=device, dtype=torch.float32)  # [13]\n",
    "            # Expand cond to per-point\n",
    "            cond_b  = cond.unsqueeze(0).expand(coords.size(0), -1)  # [N,13]\n",
    "\n",
    "            # Process in chunks to avoid OOM\n",
    "            N = coords.size(0)\n",
    "            for s in range(0, N, chunk_size):\n",
    "                e = min(s + chunk_size, N)\n",
    "                preds = model(coords[s:e], cond_b[s:e])      # [M,3], normalized\n",
    "                # Unnormalize to physical units\n",
    "                p_u = preds * out_sd + out_mu                # [M,3]\n",
    "                t_u = targets[s:e] * out_sd + out_mu\n",
    "\n",
    "                diff = p_u - t_u\n",
    "                sse += (diff ** 2).sum(dim=0).detach().cpu().numpy()\n",
    "                sae += diff.abs().sum(dim=0).detach().cpu().numpy()\n",
    "                sum_gt_abs += t_u.abs().sum(dim=0).detach().cpu().numpy()\n",
    "                sum_gt_sq  += (t_u ** 2).sum(dim=0).detach().cpu().numpy()\n",
    "                total_pts  += (e - s)\n",
    "\n",
    "    eps   = 1e-20\n",
    "    mse   = sse / max(total_pts, 1)\n",
    "    mae   = sae / max(total_pts, 1)\n",
    "    rel1  = sae / (sum_gt_abs + eps)\n",
    "    rel2  = sse / (sum_gt_sq  + eps)\n",
    "\n",
    "    keys = [\"cp\", \"cf_x\", \"cf_z\"]\n",
    "    return (\n",
    "        dict(zip(keys, mse)),\n",
    "        dict(zip(keys, mae)),\n",
    "        dict(zip(keys, rel1)),\n",
    "        dict(zip(keys, rel2)),\n",
    "        total_pts,\n",
    "        time.time() - t_start\n",
    "    )\n",
    "\n",
    "# ====================== MAIN =============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Device: {DEVICE} | CHUNK_SIZE={CHUNK_SIZE}\")\n",
    "\n",
    "    # 1) Load (or compute) normalization stats\n",
    "    NORM = load_norm_stats_or_compute(TRAIN_CSV, TRAIN_H5, NORM_JSON)\n",
    "\n",
    "    # 2) Build TEST dataset with the SAME stats (no randomness here)\n",
    "    #    NOTE: mode='test' formats case keys as case_000, case_001, ...\n",
    "    test_ds = UnifiedDesignDataset(csv_path=TEST_CSV,\n",
    "                                   hdf5_path=TEST_H5,\n",
    "                                   norm_stats=NORM,\n",
    "                                   mode=\"test\")\n",
    "    print(f\"Test designs (meshes): {len(test_ds)}\")\n",
    "\n",
    "    # 3) Load model\n",
    "    model = load_model(MODEL_PATH, device=DEVICE)\n",
    "\n",
    "    # 4) Full, all-point evaluation\n",
    "    mse, mae, rel_l1, rel_l2, npts, dt = compute_test_metrics_full(model, test_ds, device=DEVICE)\n",
    "\n",
    "    # 5) Pretty print\n",
    "    print(f\"\\nEvaluated {npts:,} points across all cases in {dt/60:.1f} min\\n=== TRUE shape metrics (test, ALL points) ===\")\n",
    "    for k in [\"cp\", \"cf_x\", \"cf_z\"]:\n",
    "        print(f\"{k:4s} | MSE={mse[k]:.6e}  MAE={mae[k]:.6e}  RelL1={rel_l1[k]:.6e}  RelL2={rel_l2[k]:.6e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
